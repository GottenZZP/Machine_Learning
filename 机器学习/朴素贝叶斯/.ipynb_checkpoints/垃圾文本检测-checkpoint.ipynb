{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47593c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fcabdd",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425128d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"你是一个愚蠢的笨蛋\",\n",
    "        \"我的狗有跳蚤，请帮助我\",\n",
    "        \"它也许不会去狗狗公园，笨蛋\",\n",
    "        \"我的玩偶也太可爱了，我爱它\",\n",
    "        \"请停止粘贴这些愚蠢且无价值的垃圾\",\n",
    "        \"那位先生正在吃我的牛排，如何去阻止他\",\n",
    "        \"请停止购买无价值的狗粮，笨蛋\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf996e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(text):\n",
    "    dataset = []\n",
    "    for i in text:\n",
    "        temp = list(jieba.cut(i, cut_all=False))\n",
    "        for j in temp:\n",
    "            if j == '，':\n",
    "                temp.remove(j)\n",
    "        dataset.append(temp)\n",
    "    classvec = [1, 0, 1, 0, 1, 0, 1]\n",
    "    return dataset, classvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c7fa914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/ck/5sxp2x1n4d534l6w14v0mvyr0000gn/T/jieba.cache\n",
      "Loading model cost 0.872 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset, classvec = loadDataSet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01356066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['你', '是', '一个', '愚蠢', '的', '笨蛋'],\n",
       " ['我', '的', '狗', '有', '跳蚤', '请', '帮助', '我'],\n",
       " ['它', '也许', '不会', '去', '狗狗', '公园', '笨蛋'],\n",
       " ['我', '的', '玩偶', '也', '太', '可爱', '了', '我', '爱', '它'],\n",
       " ['请', '停止', '粘贴', '这些', '愚蠢', '且', '无', '价值', '的', '垃圾'],\n",
       " ['那位', '先生', '正在', '吃', '我', '的', '牛排', '如何', '去', '阻止', '他'],\n",
       " ['请', '停止', '购买', '无', '价值', '的', '狗', '粮', '笨蛋']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a88b42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de1291e",
   "metadata": {},
   "source": [
    "## 创建词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5693fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataset):\n",
    "    vocabset = set()\n",
    "    for doc in dataset:\n",
    "        vocabset |= set(doc)\n",
    "    vocablist = list(vocabset)\n",
    "    return vocablist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818989d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '也', '我', '价值', '可爱', '粘贴', '购买', '垃圾', '狗狗', '请', '如何', '先生', '狗', '跳蚤', '他', '牛排', '一个', '不会', '玩偶', '也许', '去', '吃', '帮助', '无', '公园', '这些', '那位', '正在', '了', '是', '愚蠢', '你', '阻止', '它', '爱', '有', '笨蛋', '停止', '太', '且', '粮']\n"
     ]
    }
   ],
   "source": [
    "vocablist = createVocabList(dataset)\n",
    "print(vocablist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540e7f4",
   "metadata": {},
   "source": [
    "## 创建词向量（one-hot）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda26449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(vocablist, inputset):\n",
    "    returnvec = [0] * len(vocablist)\n",
    "    for word in inputset:\n",
    "        if word in vocablist:\n",
    "            returnvec[vocablist.index(word)] = 1\n",
    "        else:\n",
    "            print(f\"{word}不在词汇表中\")\n",
    "    return returnvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075e9295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my不在词汇表中\n",
      "dog不在词汇表中\n",
      "has不在词汇表中\n",
      "flea不在词汇表中\n",
      "problems不在词汇表中\n",
      "help不在词汇表中\n",
      "please不在词汇表中\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "returnvec = word2vec(vocablist, ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'])\n",
    "print(returnvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfdf6a9",
   "metadata": {},
   "source": [
    "## 获取所有词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "460955ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainMat(dataset):\n",
    "    trainmat = []\n",
    "    vocablist = createVocabList(dataset)\n",
    "    for inputset in dataset:\n",
    "        returnvec = word2vec(vocablist, inputset)\n",
    "        trainmat.append(returnvec)\n",
    "    return trainmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72869d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0], [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "trainmat = getTrainMat(dataset)\n",
    "print(trainmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f292eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee6c7955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainmat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039c296",
   "metadata": {},
   "source": [
    "## 训练朴素贝叶斯\n",
    "pAb是侮辱性词汇所占的比重，为1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83b552f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB(trainmat, classvec):\n",
    "    # 训练集长度\n",
    "    n = len(trainmat)\n",
    "    # 词向量长度\n",
    "    m = len(trainmat[0])\n",
    "    # 正例所占比重\n",
    "    pAb = sum(classvec) / n\n",
    "    # 出现在正例和反例中的所有词汇的向量初始化\n",
    "    p0Num = np.ones(m)\n",
    "    p1Num = np.ones(m)\n",
    "    # 正例和反例分别除以出现词汇的总数的初始化\n",
    "    p0Denom = (n * pAb)\n",
    "    p1Denom = 1 - (n * pAb)\n",
    "    for i in range(n):\n",
    "        if classvec[i] == 1:\n",
    "            # 将所有属于正例的词向量按位数相加，即可得到出现在正例中的所有词汇以及该词汇所出现的次数\n",
    "            p1Num += trainmat[i]\n",
    "            # 将所有属于正例的词向量内的元素相加，即可得到一个出现在所有正例词汇的总数\n",
    "            p1Denom += sum(trainmat[i])\n",
    "        else:\n",
    "            # 反例同上\n",
    "            p0Num += trainmat[i]\n",
    "            p0Denom += sum(trainmat[i])\n",
    "    # 用正例累加的所有词向量除以正例所有词向量所出现的词汇的总和既是每次词汇所属于正例的概率\n",
    "    p1v = np.log(p1Num / p1Denom)\n",
    "    p0v = np.log(p0Num / p0Denom)\n",
    "    return p0v, p1v, pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a9a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0v, p1v, pAb = trainNB(trainmat, classvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2504c78e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['的', '也', '我', '价值', '可爱', '粘贴', '购买', '垃圾', '狗狗', '请', '如何', '先生', '狗', '跳蚤', '他', '牛排', '一个', '不会', '玩偶', '也许', '去', '吃', '帮助', '无', '公园', '这些', '那位', '正在', '了', '是', '愚蠢', '你', '阻止', '它', '爱', '有', '笨蛋', '停止', '太', '且', '粮']\n"
     ]
    }
   ],
   "source": [
    "print(vocablist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c43660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.04769284, -2.74084002, -2.04769284, -3.4339872 , -2.74084002,\n",
       "       -3.4339872 , -3.4339872 , -3.4339872 , -3.4339872 , -2.74084002,\n",
       "       -2.74084002, -2.74084002, -2.74084002, -2.74084002, -2.74084002,\n",
       "       -2.74084002, -3.4339872 , -3.4339872 , -2.74084002, -3.4339872 ,\n",
       "       -2.74084002, -2.74084002, -2.74084002, -3.4339872 , -3.4339872 ,\n",
       "       -3.4339872 , -2.74084002, -2.74084002, -2.74084002, -3.4339872 ,\n",
       "       -3.4339872 , -3.4339872 , -2.74084002, -2.74084002, -2.74084002,\n",
       "       -2.74084002, -3.4339872 , -3.4339872 , -2.74084002, -3.4339872 ,\n",
       "       -3.4339872 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "532ba834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.98100147, -3.36729583, -3.36729583, -2.26868354, -3.36729583,\n",
       "       -2.67414865, -2.67414865, -2.67414865, -2.67414865, -2.26868354,\n",
       "       -3.36729583, -3.36729583, -2.67414865, -3.36729583, -3.36729583,\n",
       "       -3.36729583, -2.67414865, -2.67414865, -3.36729583, -2.67414865,\n",
       "       -2.67414865, -3.36729583, -3.36729583, -2.26868354, -2.67414865,\n",
       "       -2.67414865, -3.36729583, -3.36729583, -3.36729583, -2.67414865,\n",
       "       -2.26868354, -2.67414865, -3.36729583, -2.67414865, -3.36729583,\n",
       "       -3.36729583, -1.98100147, -2.26868354, -3.36729583, -2.67414865,\n",
       "       -2.67414865])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ea7be",
   "metadata": {},
   "source": [
    "## 套用朴素贝叶斯公式\n",
    "这里的**vec2Classify * p1v**意思就是：在是侮辱性语言的条件概率下，**vec2Classify**的概率。而整个reduce的意思就是朴素贝叶斯里的条件独立性的体现，比如$P\\left(数学好\\ 英语不好\\ 代码弱 \\mid 是\\right)=P\\left(数学好\\mid 是\\right)P\\left(英语不好\\mid 是\\right)P\\left(代码弱\\mid 是\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f7afb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0v, p1v, pAb):\n",
    "    p1 = reduce(lambda x, y: x * y, vec2Classify * p1v) * pAb\n",
    "    p0 = reduce(lambda x, y: x * y, vec2Classify * p0v) * (1 - pAb)\n",
    "    return 1 if p1 > p0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72a6c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB_1(vec2Classify, p0v, p1v, pAb):\n",
    "    p1 = sum(vec2Classify * p1v) + np.log(pAb)\n",
    "    p0 = sum(vec2Classify * p0v) + np.log(1 - pAb)\n",
    "    return 1 if p1 > p0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25de195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB(testvec):\n",
    "    dataset, classvec = loadDataSet(text)\n",
    "    vocablist = createVocabList(dataset)\n",
    "    trainmat = getTrainMat(dataset)\n",
    "    p0v, p1v, pAb = trainNB(trainmat, classvec)\n",
    "    test = word2vec(vocablist, testvec)\n",
    "    if classifyNB_1(test, p0v, p1v, pAb) == 1:\n",
    "        print(testvec, \"属于侮辱性句子\")\n",
    "    else:\n",
    "        print(testvec, \"属于非侮辱性句子\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "994614fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '玩偶'] 属于非侮辱性句子\n"
     ]
    }
   ],
   "source": [
    "testvec1 = ['我', '爱', '玩偶']\n",
    "testingNB(testvec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "845ce0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '狗'] 属于非侮辱性句子\n"
     ]
    }
   ],
   "source": [
    "testvec2 = ['我', '爱', '狗']\n",
    "testingNB(testvec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7ea0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25db28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf5166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
